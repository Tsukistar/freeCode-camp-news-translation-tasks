---
title: 学习 Python 中的聚类——机器学习工程手册
date: 2025-03-04T13:32:57.446Z
author: Tatev Aslanyan
authorURL: https://www.freecodecamp.org/news/author/tatevaslanyan/
originalURL: https://www.freecodecamp.org/news/clustering-in-python-a-machine-learning-handbook/
posteditor: ""
proofreader: ""
---

想要了解如何在数据中发现和分析隐藏的模式吗？聚类是无监督机器学习中一项关键技术，它能够揭示数据集中有价值的洞察，从而彻底改变您对复杂数据集的理解。

<!-- more -->

在这本全面的手册中，我们将深入探讨必知的聚类算法和技术，并以一些理论知识为基础进行讲解。然后，您将看到如何通过大量示例、Python 实现和可视化来运作这一切。

无论您是初学者还是经验丰富的数据科学家，这本手册都是掌握聚类技术的重要资源。您还可以[在这里下载手册。][1]

如果您也喜欢通过聆听学习，这里有一个 15 分钟的播客，我们会详细讨论聚类。在本集中，我们探讨了聚类的基本概念，提供了对如何将这些技术应用于现实世界数据的更深入了解。

<iframe width="100%" height="152" src="https://open.spotify.com/embed/episode/2O3KSW25GbqCJXl6LfUmyw" title="Spotify embed" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture" allowfullscreen="" loading="lazy"></iframe>

### 我们将涵盖以下内容：

1.  [无监督学习简介][2]
    
2.  [监督学习与无监督学习][3]
    
3.  [重要术语][4]
    
4.  [如何为无监督学习准备数据][5]
    
5.  [聚类概述][6]
    
6.  [K-Means 聚类][7]
    
    -   [K-Means 聚类：Python 实现][8]
        
    -   [K-Means 聚类：可视化][9]
        
7.  [确定最佳聚类数 (K) 的肘部法则][10]
    
8.  [层次聚类][11]
    
    -   [层次聚类：Python 实现][12]
        
    -   [层次聚类：可视化][13]
        
9.  [DBSCAN 聚类][14]
    
    -   [DBSCAN 聚类：Python 实现][15]
        
    -   [DBSCAN 聚类：可视化][16]
        
10.  [如何使用 t-SNE 进行聚类可视化][17]
    
11.  [更多无监督学习技巧][18]
    

### **通过本书，您将能够：**

1.  **理解无监督学习的基础**——您将掌握监督学习与无监督学习之间的主要区别，以及聚类在机器学习广泛领域中的地位。
    
2.  **掌握重要的聚类术语**——您将熟悉如数据点、质心、距离度量和聚类评估方法等基本概念。
    
3.  **为聚类准备数据**——您将学习如何处理缺失值、标准化数据集、去除异常值，并应用降维技术如 PCA 和 t-SNE。
    
4.  **深入理解聚类技术**——您将探索各种聚类方法，包括 K-Means、层次聚类和 DBSCAN，并了解何时使用每种方法。
    
5.  **在 Python 中实现 K-Means 聚类**——您将学习如何使用 Python 应用 K-Means 算法，使用肘部法则优化聚类数量，并有效地可视化聚类结果。
    
6.  **应用层次聚类**——您将理解聚合聚类和分裂聚类，学习如何构建树状图，并使用 Python 实现层次聚类。
    
7.  **使用 DBSCAN 进行基于密度的聚类**——您将掌握 DBSCAN 的聚类方法，包括其识别噪声点和任意形状聚类的能力。
    
8.  **可视化聚类结果**——您将能够使用 Matplotlib、Seaborn 和 t-SNE 生成聚类结果的有意义可视化，以有效分析和解释数据。
    
9.  **评估聚类性能**——您将学习如何使用轮廓系数、戴维森堡丁指数和卡林斯基哈拉巴斯指数等技术评估聚类质量。
    
10.  **使用真实世界的数据集**——您将获得将聚类技术应用于真实世界数据集（包括客户细分、异常检测和模式识别）的实践经验。
    
11.  **扩展您在聚类之外的知识**——您将被介绍到其他无监督学习技术，如混合模型和主题建模，扩展您在机器学习中的专业知识。
    

通过本手册的学习，您将对聚类和无监督学习有一个强有力的基础，这将使您能够自信地分析复杂数据集并揭示隐藏的模式！

### **先决条件**

在深入学习这本关于聚类和无监督学习的手册之前，您应该具备对机器学习概念、数据预处理技术和基础 Python 编程技能的扎实理解。这些先决条件将帮助您理解全书中介绍的理论基础和实践实现。

概念如数据点、特征、距离度量（欧几里得、曼哈顿）和相似性度量在聚类算法中发挥着重要作用。对概率、统计和线性代数的基本理解也是有益的，因为这些数学概念是许多机器学习模型的基础。

接下来，**数据预处理技术**是处理现实世界数据集的关键。由于聚类算法高度依赖于结构良好的数据，您需要了解如何处理缺失值、规范化或标准化数值特征，以及去除可能扭曲聚类结果的异常值。

诸如特征缩放（最小最大规范化、标准化）和降维（PCA, t-SNE）等技术可以提高聚类的准确性和效率，使您更容易解释结果。

最后，遵循本手册中的实际操作，**掌握Python编程和数据科学库**是必需的。您应该熟练使用NumPy和Pandas进行数据操作，使用Matplotlib和Seaborn进行可视化，并使用Scikit-learn实现机器学习算法。

由于您将应用诸如K-Means、层次聚类和DBSCAN的聚类技术，熟悉使用Jupyter Notebooks编写和执行Python脚本以及解释聚类的输出，将提升您的学习体验。

通过在这些领域打下坚实的基础，您将准备好释放聚类的强大力量，并从您的数据中获得更深入的见解。

## **无监督学习简介**

无监督学习是机器学习中的一项强大技术。它使我们能够在没有任何预定义标签或目标变量的情况下，揭示数据中的隐藏模式和结构。与依赖标记数据进行训练的监督学习不同，无监督学习让我们可以探索并理解未标记数据集中的固有结构。

无监督学习的一个关键应用是聚类。聚类是根据数据点的内在特征和相似性对相似的数据点进行分组的过程。通过识别数据集中的模式和关系，聚类帮助我们获得有价值的见解，并理解复杂数据。

聚类在各个领域中具有重要意义，包括客户细分、异常检测、图像识别和推荐系统。它使我们能够识别数据中的不同群组，将数据分类为有意义的类别，并理解推动数据集的潜在趋势。

在接下来的章节中，我们将深入探讨不同的聚类算法，如K-Means、层次聚类和DBSCAN，探索它们的理论、实现和可视化。通过本手册的学习，您将对无监督学习有一个全面的理解，并具备应用各种聚类技术进行数据分析任务的知识和技能。

请记住，聚类只是无监督学习的一个方面，它还提供了一系列其他技术和应用。那么，让我们沉浸于无监督学习的激动人心的世界中，探索它从未标记数据中提取见解的力量。

[![监督学习与无监督学习的区别 ](https://dataexpertise.in/wp-content/uploads/2023/12/Supervised-vs.-Unsupervised-Learning-1.jpg)][19]

## 监督学习与无监督学习

在机器学习中，主要有两种方法：监督学习和无监督学习。理解这两种方法之间的差异对于选择适合您的数据分析需求的正确技术至关重要。

顾名思义，监督学习涉及在标记数据上训练机器学习模型。在这种方法中，输入数据由特征（也称为属性或变量）和相应的目标值或标签组成。模型从这些标记数据中学习，并根据新的、未见过的数据进行预测或分类。

另一方面，无监督学习专注于探索未标记的数据。在无监督学习中，数据没有预定义的标签或目标值。相反，算法自行搜索数据中的模式、结构和关系。其目标是发现隐藏的见解，深入理解数据的内在结构。

无监督学习的一大优势是能够发现以前未知的模式和关系。在没有标记数据的限制下，无监督算法可以揭示通过其他分析方法可能无法显现的重要见解。这使得无监督学习在探索性数据分析、异常检测和聚类中特别有用。

在监督学习中，目标变量为学习过程提供指导作用，使模型能够进行准确的预测或分类。但这种对标记数据的依赖也可能限制模型的能力，因为它可能会在训练数据中未出现的新模式上表现不佳。

总之，监督学习非常适合用于有标签数据且目标是做出精确预测或分类的任务。另一方面，当在数据中探索隐藏模式和关系，尤其是在标签数据稀缺或不存在的情况下，无监督学习显得尤为有价值。

通过理解这两种方法的区别，你可以有效地选择正确的技术，以释放数据分析工作的全部潜力。

## **重要术语**

为了充分理解无监督学习和聚类，熟悉与这些概念相关的关键术语是至关重要的。以下是一些你应该知道的重要术语：

**1\. 数据点**

数据点指的是数据集中一个单独的观察或实例。每个数据点包含描述特定对象或事件的各种特征或属性。

**2\. 聚类数量**

聚类数量表示在聚类过程中数据将被划分为的不同组的期望或估计数量。它是决定结果聚类结构的一个重要参数。

**3\. 无监督算法**

无监督算法是一种数学程序，用于在无需标签或预分类示例的情况下识别数据中的模式或关系。这些算法探索数据集的内在结构和复杂性，以发现隐藏的见解。

理解并利用这些术语将为你进入无监督学习和聚类的旅程奠定坚实的基础。在接下来的章节中，我们将深入探讨在Python中聚类技术的实际应用和实施。

[![图像说明了从数据收集到清洗、转换、降维和拆分的数据准备过程。来自机器学习的数据准备：终极指南 | Pecan AI](https://cdn.letterdrop.co/pictures/fe3db832-862f-4a35-be7c-37231ad814bb.png)][20]

## **如何为无监督学习准备数据**

在实施无监督学习算法之前，确保数据得到适当的准备是至关重要的。这涉及到采取某些步骤来优化输入数据，使其适合于使用聚类技术进行分析。在为无监督学习准备数据时，有以下重要考量：

### **数据规范化**

数据准备的一个关键方面是规范化，即所有特征被缩放到一个一致的范围内。这是必要的，因为数据集中的变量可能具有不同的单位或尺度。

规范化有助于在聚类过程中避免对任何特定特征产生偏向。常见的规范化方法包括最小-最大缩放和标准化。

### **处理缺失值**

处理缺失值是另一个关键步骤。重要的是在应用聚类算法之前识别和解决数据集中的任何缺失值。

处理缺失值的方法有多种，例如插补，在这种方法中，缺失值由基于统计方法或算法的估计值代替。

### **异常值检测与处理**

异常值可以显著影响聚类结果，因为它们可能会影响聚类边界的确定。因此，适当地检测和处理异常值是至关重要的。这可以涉及使用诸如Z分数或四分位距（IQR）分析等技术来识别和处理异常值。

### **降维**

在某些情况下，数据集可能具有高维特性，即包含大量特征。高维数据可能难以有效地可视化和分析。降维技术，例如主成分分析（PCA），可以用于在保留数据中最信息丰富方面的同时减少特征数量。

通过仔细准备数据、规范化变量、处理缺失值、解决异常值并在必要时降维，你可以优化用于无监督学习算法的输入数据质量。这确保了准确且有意义的聚类结果，进而导致数据内有价值的见解和模式。

请记住，数据准备是无监督学习过程中一个关键步骤，为成功的聚类分析奠定基础。

[![K-Means聚类的可视化，彩色数据点在坐标平面上按簇排列。周围是说明簇分配和质心的图示和数学公式。 - Analytics Vidhya](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/An-Introduction-to-K-Means-Clustering-.webp)][21]

## **聚类解释**

聚类是无监督学习中的一项基本技术，在揭示数据中的隐藏模式方面起着至关重要的作用。它包括基于相似性对数据点进行分组，使我们能够识别数据集中的不同子集或簇。通过分析这些簇的结构，我们可以获得有价值的见解并以数据为导向做出决策。

在其核心，聚类的目标是在没有任何预定义标签或目标变量的情况下，寻找数据点之间的相似性或关系。目标是最大化每个簇内的相似性，同时最大化不同簇之间的差异性。这一过程使我们能够识别数据中的模式和内在结构。

簇可以根据各种因素来定义，例如距离、连通性或密度。每个簇中的数据点与同一簇内的其他点相比，与其他簇中的点有更多的相似之处。这种分组使我们能够细分数据，这在客户细分、异常检测和图像识别等各个领域非常有用。

### **聚类算法的类型**

有几种不同的聚类算法可用，每种算法都有自己的数据分区方法。一些流行的包括 K-Means 聚类、层次聚类和 DBSCAN（基于密度的噪声应用空间聚类）。

#### **1\. K-Means 聚类**

K-Means 聚类是一种广泛使用的算法，旨在将数据划分为 K 个不同的簇。它通过反复将每个数据点分配到最近的簇中心，然后重新计算中心点来运作。此过程持续进行直到收敛，产生明确定义的簇。

#### **2\. 层次聚类**

层次聚类通过根据特定标准递归地分割或合并簇来创建一个簇的层次结构。这种方法可以用树状图表示，提供了关于簇之间层次结构和关系的有价值的见解。

#### **3\. DBSCAN 聚类**

DBSCAN 是一种基于密度的算法，根据数据点的密度和连通性将它们分组。它在识别任意形状的簇和处理噪声数据方面特别有效。

这些只是聚类算法的一些例子，每种算法在特定场景下都有其自身的优势和适用性。根据数据特征和问题领域选择最合适的算法非常重要。

在接下来的部分中，我们将更深入地探讨这些聚类算法的理论、实现和可视化，以便为您提供对其工作原理和使用时机的全面了解。

请记住，聚类是一种强大的技术，可以让我们解锁数据中隐藏的结构，从而获得有价值的见解和做出明智的决策。让我们进入聚类的世界，发现它所蕴含的潜力。

[![K-Means Clustering — The Science of Machine Learning & AI](https://images.squarespace-cdn.com/content/v1/5acbdd3a25bf024c12f4c8b4/1608407348392-22767PJ7RQ85BD5RLSLZ/k-means-clustering.png)][22]

## **K-Means 聚类**

K-Means 聚类是一种流行的无监督学习算法，用于基于相似性将数据点划分为不同的组。在本节中，我们将深入探讨 K-Means 聚类的理论，并使用 scikit-learn 库在 Python 中实现它。

在数据科学和数据分析中，我们经常希望将观测数据分类为一组**段**或**簇**，以实现不同的目的。例如，公司可能希望根据客户的交易历史或购买频率，将他们的客户分成 3-5 个组。这通常是一种**无监督学习**方法，其中标签（组/段/簇）是未知的。

K-Means 是将观测数据分组的最流行的聚类方法之一，它是一种无监督聚类算法。K-Means 聚类的条件如下：

-   需要提前指定簇的数量：K
    
-   每个观测需要至少属于一个类
    
-   每个观测仅能属于一个类（类需要是不重叠的）
    
-   没有一个观测应该属于多个类
    

K-Means 的基本理念是**最小化簇内变化并最大化簇间变化。**因此，K-Means 将观测数据分为 K 个簇，使得所有 K 个簇的总簇内变化尽可能小。

其动机在于对观测数据进行聚类，使同一组中的观测数据尽可能相似，而不同组的观测数据尽可能不同。

在数学上，簇内变化是基于您自选的距离测度来定义的。例如，您可以选择使用欧几里得距离、曼哈顿距离等作为距离测度。

当簇内变化最小时，K-Means 聚类是最优的。C\_k 簇的簇内变化是衡量簇内观测数据相互之间差异的指标 W(C\_k)。因此，应解决以下优化问题：

$$\\min\_{C\_1, \\dots, C\_K} \\sum\_{k=1}^{K} W(C\_k)$$

使用欧几里得距离的簇内变化可以表示为：

$$W(C\_k) = \\frac{1}{|C\_k|} \\sum\_{i,i' \\in C\_k} \\sum\_{j=1}^{p} (x\_{ij} - x\_{i'j})^2$$

$$\\min\_{C\_1, \\dots, C\_K} \\left\\{ \\sum\_{k=1}^{K} \\frac{1}{|C\_k|} \\sum\_{i,i' \\in C\_k} \\sum\_{j=1}^{p} (x\_{ij} - x\_{i'j})^2 \\right\\}$$

### **K-Means 算法**

K-means 算法的伪代码可以描述如下：

[![替代文本：图片展示了 K-means 算法的伪代码，包含两个主要步骤。步骤 1：根据初始条件随机将每个数据点分配到一个集群。步骤 2：当集群发生变化时，更新集群质心，并重新分配点直到收敛。](https://miro.medium.com/v2/resize:fit:1400/1*0DjFFWY4tY74Z8EMXggEMA.png)][23]

K-Means 是一种非确定性方法，其随机性体现在步骤 1，其中所有观察数据随机分配到 K 个类别之一。

在第二步中，对于每个集群，通过计算集群中所有数据点的均值来计算集群质心。第 _K_ 个集群的质心是一个长度为 _p_ 的向量，包含了 _k_ 集群中观测值的所有变量的均值，其中 _p_ 是变量的数量。

然后，在下一步中，通过反复最小化**总组内平方和**更新观测值的集群，使得每个观测值被分配到质心最近的集群中。我们重复步骤 2 和步骤 3，直到集群质心不再变化或达到最大迭代次数。

### **K-Means 聚类：Python 实现**

让我们来看一个示例，我们的目标是将观测值分为 4 类。原始数据如下所示：

[![标题为"原始数据可视化"的散点图，显示绿色圆点沿 X 值从 0 到 3 的列排列，Y 值在 0 到 10 之间。](https://miro.medium.com/v2/resize:fit:1200/1*QRRqHu4MATa7piwcPHmsSA.png)][24]

```
# 导入必要的库
# KMeans 是来自 scikit-learn 的聚类算法
from sklearn.cluster import KMeans  
# Metrics 模块用于评估聚类性能
from sklearn import metrics  
# NumPy 用于数值计算和数组操作
import numpy as np  
# Pandas 用于以结构化 DataFrame 格式处理数据
import pandas as pd  

# 生成用于 K-Means 聚类的合成数据
# 创建一个 100x2 的数组，元素为从 0 到 9 的随机整数
df = np.random.randint(0, 10, size=[100, 2])  
# 生成一个 300x1 的数组，元素为从 0 到 3 的随机整数
X1 = np.random.randint(0, 4, size=[300, 1])  
# 生成一个 300x1 的数组，元素为从 0 到 10 的随机浮点数
X2 = np.random.uniform(0, 10, size=[300, 1])  
# 将 X1 和 X2 沿第二个轴组合为一个具有两个特征的数据集
df = np.append(X1, X2, axis=1)  

# 在生成的数据集上应用 K-Means 聚类算法
# 调用 KMeans_Algorithm 函数，设置 K=4 个集群
Clustered_df = KMeans_Algorithm(df=df, K=4)  
# 将聚类后的数据转换为 Pandas DataFrame
df = pd.DataFrame(Clustered_df)  


# 执行 K-Means 聚类的函数
def KMeans_Algorithm(df, K):
    """
    在给定的数据集上执行 K-Means 聚类。

    参数：
    df (array-like)：要聚类的输入数据集。
    K (int)：集群的数量。

    返回：
    df (DataFrame)：原始数据集并附加了用于存储集群标签的新列。
    """

    # 使用指定的参数初始化 K-Means 模型
    # 将集群数设置为 K
    # 使用 k-means++ 初始化以改进收敛
    # 将最大迭代次数设置为 300
    # 设置固定随机种子以确保结果可重现
    KMeans_model = KMeans(
        n_clusters=K,  
        init='k-means++',  
        max_iter=300,  
        random_state=2021  
    )

    # 在数据集上拟合 K-Means 模型
    KMeans_model.fit(df)

    # 提取集群质心（每个集群的中心点）
    centroids = KMeans_model.cluster_centers_

    # 将质心转换为具有 "X" 和 "Y" 列名的 DataFrame
    centroids_df = pd.DataFrame(centroids, columns=["X", "Y"])

    # 获取分配给每个数据点的集群标签
    labels = KMeans_model.labels_

    # 将输入数据转换为 Pandas DataFrame（如果还没有）
    df = pd.DataFrame(df)

    # 添加新列以存储分配的集群标签
    df["labels"] = labels

    # 返回附加了集群标签的更新 DataFrame
    return df
```

[![K-Means 聚类的 Python 代码截图。包括导入 scikit-learn、numpy 和 pandas 等库，生成合成数据，并定义一个带参数的函数以进行聚类和 K-Means 模型初始化。代码处理数据集并返回带集群标签的 DataFrame。- lunartech.ai](https://cdn.hashnode.com/res/hashnode/image/upload/v1738528849086/9891484a-a8b0-45eb-a8e3-f1a76c038b73.png)][25]

该脚本旨在生成合成数据，应用 K-Means 聚类，并为每个数据点分配集群标签。K-Means 聚类算法是一种无监督的机器学习方法，根据数据点在特征空间中的接近程度将其归为相似的簇。以下是该脚本如何工作的分步骤详细说明。

接下来，脚本生成合成数值数据。创建一个名为 `df` 的 NumPy 数组，其维度为 100x2，包含介于 0 和 9 之间的随机整数。此外，还单独生成了两个数组 `X1` 和 `X2`。`X1` 包含 300x1 的随机整数，范围从 0 到 3，而 `X2` 包含 300x1 的随机浮点数，范围在 0 到 10 之间。然后将这些数组沿第二个轴组合，形成一个具有两个特征的数据集，为聚类做好准备。

一旦合成数据准备就绪，脚本就会应用 K-Means 聚类算法。`KMeans_Algorithm` 函数被调用，并设置 `K=4`，这意味着算法将尝试将数据分成四个簇。函数返回聚类后的数据集，并将其转换为 Pandas DataFrame 格式。

`KMeans_Algorithm` 函数接受两个参数：数据集 `df` 和簇的数量 `K`。在该函数内部，K-Means 模型使用 `KMeans()` 进行初始化。簇的数量被设置为 `K`，同时 `init='k-means++'` 参数确保更好的初始化以加快收敛速度。`max_iter=300` 参数限制迭代次数，以避免过多的计算时间。`random_state=2021` 确保结果可重复。

初始化后，K-Means 模型使用 `KMeans_model.fit(df)` 进行拟合。这一步处理数据集，识别簇中心并相应地分组数据点。训练完成后，使用 `KMeans_model.cluster_centers_` 提取簇的质心，并将这些质心存储在 Pandas DataFrame 中，列名为 "X" 和 "Y"，以便于解释。

每个数据点被分配一个簇标签，可以使用 `KMeans_model.labels_` 获取。脚本然后确保数据集存储为 Pandas DataFrame 格式（如果尚未格式化为这种形式），并添加一个新列 `"labels"` 来存储分配的簇标签。最后，更新后的数据集，现包含原始特征和簇分配信息，将返回。

此脚本的输出是一个 Pandas DataFrame，包含三列：两个表示生成数据点的数值特征列和一个指示每个数据点簇分配的 `"labels"` 列。例如，一个输出的简化视图可能显示一行，其中值为 `[2.0, 7.4]` 的点分配到簇 `0`，而另一个值为 `[1.0, 3.2]` 的点属于簇 `1`。

该脚本成功创建了一个结构化的数据集，将数据分成四个不同的组，并为每个点分配了有意义的簇标签。可以通过散点图等可视化技术进一步分析结果，以了解聚类分布。未来可能的改进包括使用轮廓系数等指标来评估聚类质量，或尝试不同的簇数量以找到最优的分组。

### **K-Means 聚类：可视化**

K-Means 的主要优势之一在于其简单性和处理大型数据集的效率。这是一种广泛应用于各个领域的聚类算法，包括客户细分、图像压缩、异常检测和模式识别。

尽管简单，K-Means 在发现数据内在组结构方面非常有效，使其成为无监督学习中的一个基本工具。但像任何算法一样，它也有其局限性——例如对初始质心选择的敏感性以及难以检测非球形簇。理解这些优缺点将在将 K-Means 应用于真实数据集时帮助做出明智的决策。

在本节中，我们将探讨如何在 Python 中实施 K-Means 聚类并可视化结果。通过逐步的代码实现，您将看到数据点如何被分组到簇中，以及算法如何迭代地优化其簇分配。我们还将讨论选择最佳簇数量的最佳实践以及如何评估聚类质量。

### 理解 K-Means 算法

在我们深入实施之前，先简要了解一下 K-Means 算法的工作原理。该算法遵循以下步骤：

1.  **步骤 1：初始化** – 随机选择 K 个质心，其中 K 代表期望的簇数量。
    
2.  **步骤 2：分配** – 根据欧氏距离将每个数据点分配给最近的质心。
    
3.  **步骤 3：更新** – 通过对分配给每个簇的所有数据点取平均值来重新计算质心。
    
4.  **步骤 4：重复** – 重复步骤 2 和步骤 3，直到满足收敛标准（例如，质心移动最小）。
    

```
fig, ax = plt.subplots(figsize=(6, 6))

# 对于列 1 和 2 中每种标签类型的观测
plt.scatter(df[df["labels"] == 0][0], df[df["labels"] == 0][1],
c='black', label='cluster 1')
plt.scatter(df[df["labels"] == 1][0], df[df["labels"] == 1][1],
c='green', label='cluster 2')
plt.scatter(df[df["labels"] == 2][0], df[df["labels"] == 2][1],
c='red', label='cluster 3')
plt.scatter(df[df["labels"] == 3][0], df[df["labels"] == 3][1],
c='y', label='cluster 4')
plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='black', label='centroid')
plt.legend()
plt.xlim([-2, 6])
plt.ylim([0, 10])
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Visualization of clustered data')
ax.set_aspect('equal')
plt.show()
```

[![标题为“数据点聚类可视化”的散点图，显示四个不同颜色的集群：第1集群为黑色，第2集群为绿色，第3集群为红色，第4集群为黄色。黑色星星标记在图中的中心点，X轴和Y轴分别从-2到6和0到10。图中包含图例。](https://miro.medium.com/v2/resize:fit:1400/1*Isl-76ShvTNwa35Xu50yHA.png)][28]

在上图中，K-means将这些观测值聚类成了4组。正如你从图中所看到的，通过图表的方式聚类观测值看起来很自然并且确实合情合理。

### **确定集群最佳数量 (K) 的肘部法则**

使用K-means的最大挑战之一是选择集群数量。有时这是一个业务决策，但大多数时候我们希望选择一个最佳且合乎逻辑的K。确定K或集群数量最优值的最流行方法之一是**肘部法则**。

使用这种方法，你需要知道什么是**惯性**。惯性是样本到其最近的集群中心的平方距离之和。因此，惯性或**集群内平方和**值指示不同集群的凝聚程度或纯净程度。惯性可以描述如下：

$$\\sum\_{i=1}^{N} (x\_i - C\_k)^2$$

其中N是数据集内的样本数，C是集群中心，k是集群索引。因此，惯性简单地计算集群内每个样本到其集群中心的平方距离并将它们相加。

然后我们可以计算不同集群数量K的惯性。我们可以像下图那样绘制，考虑K = 1,2,….,10。然后从图中我们可以选择惯性出现肘部的位置对应的K。在这种情况下，肘部出现在K = 3。

[![折线图展示K-Means肘部法则，x轴为1到9的集群数量，y轴为惯性，图中显示在集群3附近惯性急剧下降。](https://miro.medium.com/v2/resize:fit:1400/1*S9wmsHzA4nVnZ7zSi9WfLA.png)][29]

```
def Elbow_Method(df):
    inertia = []
    # 考虑 K = 1,2,...,10 作为 K
    K = range(1, 10)
    for k in K:
        KMeans_Model = KMeans(n_clusters=k, random_state = 2022)
        KMeans_Model.fit(df)
        inertia.append(KMeans_Model.inertia_)
    return(inertia)

K = range(1, 10)
inertia = Elbow_Method(df)
plt.figure(figsize = (17,8))
plt.plot(K, inertia, 'bx-')
plt.xlabel("K: 集群数量")
plt.ylabel("惯性")
plt.title("K-Means: 肘部法则")
plt.show()
```

[![代码片段显示使用Python实现K-Means算法的肘部法则。函数计算1到10个集群数量的惯性值，并通过Matplotlib绘制结果以识别最佳集群数。- lunartech.ai](https://cdn.hashnode.com/res/hashnode/image/upload/v1738529158688/f8c4892b-962b-416d-9795-c442b149deee.png)][30]

K-Means是一种非确定性的方法，它的随机性来自于步骤1，在这一步中，所有观测值随机分配到K个分类中的一个。

如你所见，K-Means聚类提供了一种根据相似性将数据点分组的高效且有效的方法。通过在Python中实现K-Means算法，可以轻松将此技术应用于自己的数据集并获得有价值的数据洞察。

Python提供了实施和可视化K-Means聚类的强大工具。借助scikit-learn库和matplotlib，你可以轻松地将K-Means应用到你的数据集中，并从中学到很多东西。

[![层次聚类中距离矩阵比较的图示。图中展示了四种方法：最小值法、最大值法、组平均法和Ward法，每种方法用圆和编号点表示数据集群。](https://media.geeksforgeeks.org/wp-content/uploads/20230427165259/Distance-Matrix-in-Hierarchical--Clustering.webp)][31]

## **层次聚类理论**

另一种流行的聚类技术是层次聚类。这是另一种无监督学习技术，可以帮助我们将观测值聚类到不同的片段中。但不同于K-means，层次聚类开始时将每个观测值视为一个独立的集群。

### **凝聚聚类与分裂聚类**

层次聚类主要有两种类型：凝聚聚类和分裂聚类。

凝聚聚类开始时将每个数据点分配到自己的集群中。然后，它根据选定的距离度量迭代合并最相似的集群，直到形成包含所有数据点的单个集群。

这种自下而上的方法创建一个类似二叉树的结构，也称为树状图，其中每个节点的高度代表合并的集群之间的差异。

另一方面，分裂聚类从一个包含所有数据点的单个集群开始。然后递归地将集群划分成更小的子集群，直到每个数据点都位于自己的集群中。这种自上而下的方法生成一个树状图，以提供对集群层次结构的见解。

要确定聚类或数据点之间的相似度，可以使用各种距离度量。常用的距离度量包括欧几里得距离、曼哈顿距离和余弦相似度。这些度量量化了数据点对之间的异质性或相似度，并指导聚类过程。

在这种技术中，最初每个数据点都被视为一个独立的聚类。在每次迭代中，最相似或最不异质的聚类合并为一个聚类，这个过程持续进行，直到只剩下一个聚类。因此，该算法重复执行以下步骤：

- 1: 找出最接近的两个聚类
- 2: 合并两个最相似的聚类。

然后继续此迭代过程，直到将所有聚类合并在一起。

两个聚类之间异质性或相似度的计算取决于我们假定的连接类型。这里有五个流行的连接选项：

- **完全连接（Complete Linkage）:** 最大聚类间异质性，需要计算聚类K1中观测值与聚类K2中观测值之间的所有成对异质性。然后选择其中最大的相似度。
- **单一连接（Single Linkage）:** 最小聚类间异质性，需要计算聚类K1中观测值与聚类K2中观测值之间的所有成对异质性。然后选择其中最小的相似度。
- **平均连接（Average Linkage）:** 平均聚类间异质性，需要计算聚类K1中观测值与聚类K2中观测值之间的所有成对异质性。然后计算这些相似度的平均值。
- **质心连接（Centroid Linkage）:** 计算聚类K1质心和聚类K2质心之间的异质性（这通常是较不理想的连接选择，因为它可能导致大量重叠）。
- **Ward 方法（Ward’s method）:** 根据减少每个观测值与聚类中平均观测值之间的平方距离之和来确定合并哪些观测值为聚类。

### **层次聚类 Python 实现**

层次聚类是一种强大的无监督学习技术，允许您根据相似度将数据点分组成聚类。在本节中，我们将探讨使用 Python 实现层次聚类。

下面是如何使用 Python 实现层次聚类的示例代码：

```python
import scipy.cluster.hierarchy as HieraarchicalClustering
from sklearn.cluster import AgglomerativeClustering
import numpy as np
import pandas as pd

# 创建层次聚类的数据
df = np.random.randint(0,10,size = [100,2])
X1 = np.random.randint(0,4,size = [300,1])
X2 = np.random.uniform(0,10,size = [300,1])
df = np.append(X1,X2,axis = 1)
hierCl = HieraarchicalClustering.linkage(df, method='ward')

Hcl= AgglomerativeClustering(n_clusters = 7, affinity = 'euclidean', linkage ='ward')
Hcl_fitted = Hcl.fit_predict(df)
df = pd.DataFrame(df)
df["labels"] = Hcl_fitted
```

![Python代码的截图，其中使用库如scipy，sklearn，numpy和pandas进行层次聚类。代码生成随机数据，使用函数进行聚类，并输出带标签的数据框。 - lunartech.ai](https://cdn.hashnode.com/res/hashnode/image/upload/v1738529216677/9b71d1c5-4847-4cc3-b847-0620409119d6.png)

这个代码实现了使用 Scipy 的层次聚类模块和 Scikit-learn 的 Agglomerative Clustering 算法进行的层次聚类。脚本的目的是生成一个合成数据集，应用层次聚类，并为数据点分配聚类标签。

脚本的第一部分导入了必要的库。Scipy 的层次聚类模块 (`scipy.cluster.hierarchy`) 被导入为 `HieraarchicalClustering`，用于执行基于连接的聚类。Scikit-learn 的 `AgglomerativeClustering` 类也被导入，用于实现特定类型的层次聚类。同时，NumPy 用于数值操作和生成随机数据，而 Pandas 用于将数据构建为 DataFrame。

接下来，脚本生成合成数值数据。创建了一个包含随机整数的 100×2 矩阵 (`df`)，范围在 0 到 9 之间。然后，分别创建了两个数据集 `X1` 和 `X2`。`X1` 包含 300 个范围在 0 到 3 之间的随机整数，而 `X2` 包含 300 个范围在 0 到 10 之间的随机浮点数。这两个数据集然后使用 `np.append()` 沿第二个轴合并，形成一个包含两个特征的数据集，将用于聚类。

一旦数据集准备好，就使用 Ward 连接方法应用层次聚类，该法减少合并聚类之间的方差。使用 `HieraarchicalClustering.linkage(df, method='ward')` 创建连结矩阵 `hierCl`，计算层次聚类解决方案。

在生成层次聚类连接矩阵之后，应用凝聚聚类（Agglomerative Clustering）将数据分为七个聚类 (`n_clusters=7`)。参数 `affinity='euclidean'` 指定欧几里得距离作为度量点之间相似度的距离度量。参数 `linkage='ward'` 确保使用 Ward 方法以最小化方差为基础合并聚类。使用 [`Hcl.fit_predict(df)`][33] 拟合数据集模型，并为每个数据点分配聚类标签。

总结来说，这个脚本生成随机数据，使用 Scipy 的链接方法和 Scikit-learn 的凝聚层次聚类进行层次聚类，并将聚类标签分配给每个数据点。最终的数据集可用于分析聚类结构、可视化结果或验证聚类有效性。

### **层次聚类：可视化**

层次聚类的一个主要优势是其能够创建一个层次结构的聚类，这可以提供关于数据点之间关系的重要见解。

在 Python 中可使用 Scikit-learn、SciPy 和 Matplotlib 等库来可视化层次聚类。这些库提供了易于使用的函数和工具，有助于简化可视化过程。

因此，在执行层次聚类之后，通常可以通过可视化聚类来获得帮助。我们可以使用各种技术进行可视化，比如树状图或热图。

如上所述，树状图是一个类树形的图示，展示了聚类之间的层次关系。它可以通过 Python 中的 Scipy 库生成。

以下是如何在 Python 中可视化树状图和聚类点的示例：

```
# 生成一个树状图以帮助确定最佳聚类数
# 树状图可视化了层次聚类如何逐步合并点
dendrogram = HieraarchicalClustering.dendrogram(hierCl)

# 设置树状图的标题
plt.title('Dendrogram')

# 标记 x 轴以指示观测值（数据点）
plt.xlabel("Observations")

# 标记 y 轴以显示聚类之间的欧氏距离
plt.ylabel('Euclidean distances')

# 显示树状图
plt.show()


# 使用散点图可视化聚类数据
# 每种颜色代表不同的聚类

# 将属于聚类 1 的所有点绘制为黑色
plt.scatter(df[df["labels"] == 0][0], df[df["labels"] == 0][1], 
            c='black', label='cluster 1')

# 将属于聚类 2 的所有点绘制为绿色
plt.scatter(df[df["labels"] == 1][0], df[df["labels"] == 1][1], 
            c='green', label='cluster 2')

# 将属于聚类 3 的所有点绘制为红色
plt.scatter(df[df["labels"] == 2][0], df[df["labels"] == 2][1], 
            c='red', label='cluster 3')

# 将属于聚类 4 的所有点绘制为品红色
plt.scatter(df[df["labels"] == 3][0], df[df["labels"] == 3][1], 
            c='magenta', label='cluster 4')

# 将属于聚类 5 的所有点绘制为紫色
plt.scatter(df[df["labels"] == 4][0], df[df["labels"] == 4][1], 
            c='purple', label='cluster 5')

# 将属于聚类 6 的所有点绘制为黄色
plt.scatter(df[df["labels"] == 5][0], df[df["labels"] == 5][1], 
            c='y', label='cluster 6')

# 将属于聚类 7 的所有点绘制为黑色
plt.scatter(df[df["labels"] == 6][0], df[df["labels"] == 6][1], 
            c='black', label='cluster 7')

# 显示图例以标记每个聚类
plt.legend()

# 标记 x 轴表示特征 1（第一维）
plt.xlabel('X')

# 标记 y 轴表示特征 2（第二维）
plt.ylabel('Y')

# 设置散点图的标题
plt.title('Hierarchical Clustering')

# 显示聚类的散点图
plt.show()
```

[![用于在 Python 中可视化层次聚类的代码片段。它包括生成树状图和创建散点图以表示聚类，每个聚类以不同颜色显示。X 和 Y 轴进行了标记，图表标题清晰设置。代码使用 Matplotlib 函数，如 , , , 和 . - lunartech.ai](https://cdn.hashnode.com/res/hashnode/image/upload/v1738529338003/d04605b0-8c9e-46d9-8aac-0f62dc0a67d3.png)][34]

以下是 Python 中可视化层次聚类的分步指南：

**步骤 1：预处理数据**

在可视化层次聚类之前，重要的是通过缩放或标准化数据来进行预处理。这确保了所有特征具有相似的范围，并防止对某些特征产生偏见。

**步骤 2：执行层次聚类**

接下来，我们使用所选算法（如使用 Scikit-learn 的 AgglomerativeClustering）执行层次聚类。该算法计算数据点之间的相似性并根据特定的链接标准将它们合并为聚类。

**步骤 3：创建树状图**

我们可以使用 SciPy 库的树状图函数来创建这种可视化效果。树状图让我们可以可视化聚类之间的距离和关系。

**步骤 4：绘制聚类**

最后，我们可以使用散点图或其他合适的可视化技术来绘制聚类。这有助于我们可视化每个聚类中的数据点，并深入了解每个聚类的特征。

[![展示观测值层次聚类的树状图，使用欧氏距离。图表标记了聚类编号，枝条为蓝色、绿色和橙色。- lunartech.ai](https://miro.medium.com/v2/resize:fit:1400/1*wIrFoLxUBv-8Y_cuskgukQ.png)][35]

这个树状图可以帮助我们决定可以更好使用的聚类数量。如你所见，在这种情况下，似乎应该使用 7 个聚类。

通过在 Python 中可视化层次聚类，我们能够更好地理解数据的结构和关系。这种可视化技术在处理复杂数据集时特别有用，可以协助决策过程和模式发现。

请记住根据您的数据集和目标调整特定的参数和设置。尝试不同的可视化和技术可以让您对数据有更深入的见解。

## **DBSCAN 聚类理论**

DBSCAN（基于密度的噪声应用空间聚类）是一种用于聚类分析的无监督学习算法。它在识别任意形状的簇和处理噪声数据方面特别有效。

与 K-Means 或层次聚类不同，DBSCAN 不需要提前指定簇的数量。相反，它根据数据中的密度和连接性来定义簇。

### **DBSCAN 的工作原理：**

**基于密度的聚类**：DBSCAN 将相互接近且拥有足够数量的邻居的数据点分成一组。它将数据点的密集区域识别为簇，并将稀疏区域识别为噪声。

**核心点、边界点和噪声点**：DBSCAN 将数据点分为三类：核心点、边界点和噪声点。

-   核心点：在指定距离（由 `eps` 参数定义）内拥有最少邻居（由 `min_samples` 参数定义）的数据点。
    
-   边界点：在核心点的 `eps` 距离内，但没有足够邻居以被视为核心点的数据点。
    
-   噪声点：既不是核心点也不是边界点的数据点。
    

**可达性和连接性**：DBSCAN 使用可达性和连接性的概念来定义簇。如果一个数据点可以通过一条核心点路径到达另一个数据点，那么它们是可达的。如果两个数据点是可达的，它们属于同一个簇。

**簇的扩展**：DBSCAN 从一个任意数据点开始，通过检查其邻居及其邻居的邻居来扩展簇，形成一组连接的数据点。

### **DBSCAN 聚类的优点：**

-   **检测复杂结构的能力**：DBSCAN 可以发现各种形状和大小的簇，使其适用于具有非线性关系或不规则模式的数据集。
    
-   **对噪声的鲁棒性**：DBSCAN 能够有效处理噪声数据，将噪声点与簇分开。
    
-   **自动确定簇的数量**：DBSCAN 不需要提前指定簇的数量，使其更方便并可适应不同的数据集。
    
-   **可扩展到大型数据集**：与一些其他聚类算法相比，DBSCAN 的时间复杂度相对较低，使其能够良好扩展到大型数据集。
    

在接下来的部分中，我们将深入探讨如何在 Python 中实现 DBSCAN 算法，提供逐步指导和示例。

### **DBSCAN 聚类：Python 实现**

在本节中，我将指导您如何使用 Python 实现 DBSCAN。

#### DBSCAN 聚类的关键步骤

1.  **准备数据**：在应用 DBSCAN 之前，重要的是预处理数据。这包括处理缺失值、规范化特征和选择适当的距离度量。
    
2.  **定义参数**：DBSCAN 需要两个主要参数：epsilon (ε) 和最小点数(MinPts)。Epsilon 决定了两个点之间的最大距离，以将它们视为邻居，MinPts 指定了形成密集区域所需的最小点数。
    
3.  **执行基于密度的聚类**：DBSCAN 从随机选择一个数据点开始，识别在指定 epsilon 距离内的邻居。如果邻居的数量超过 MinPts 阈值，则形成一个新簇。算法通过迭代添加新点扩展这个簇，直到没有更多点可达。
    
4.  **执行噪声检测**：不属于任何簇的点被视为噪声或异常。这些点不被分配到任何簇，并可能在识别数据异常中起关键作用。
    

要在 Python 中执行 DBSCAN 聚类，我们可以使用 scikit-learn 库。第一步是导入必要的库并加载我们想要聚类的数据集。然后，我们可以创建一个 DBSCAN 类的实例，并设置 epsilon (eps) 和最小样本数 (min_samples) 参数。

以下是让您入门的示例代码片段：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN

# 生成一些示例数据
X, _ = make_moons(n_samples=500, noise=0.05, random_state=0)

# 应用 DBSCAN
db = DBSCAN(eps=0.3, min_samples=5, metric='euclidean')
y_db = db.fit_predict(X)
```

[![一段代码片段在紫色背景上展示了使用 DBSCAN 聚类算法的过程。它导入了 numpy 和 matplotlib 等库，生成了示例数据并用指定参数应用了 DBSCAN。 - lunartech.ai](https://cdn.hashnode.com/res/hashnode/image/upload/v1738529451227/4b01ac7c-a9f9-4666-8fe5-e457a18ad160.png)][37]

DBSCAN 相较于其他聚类算法有许多优势，比如不需要预先定义簇的数量。这使得它适用于簇数未知的数据集。DBSCAN 也能够识别不同形状和大小的簇，使其在捕捉复杂结构时更为灵活。

但 DBSCAN 可能在处理具有不同密度的数据集时遇到困难，并对 epsilon 和最少点数参数的选择较为敏感。将这些参数调整到最佳状态对于获得最优聚类结果至关重要。

通过在 Python 中实现 DBSCAN，你可以利用这一强大的聚类算法来发现数据中有意义的模式和结构。

在我们探讨 DBSCAN 和其他聚类技术的区别之前，让我们仔细看看影响 DBSCAN 表现和结果的关键参数。

### 理解 DBSCAN 的关键参数

**eps** (epsilon) 参数定义了两点之间的最大距离，以使其中一个点被视为另一个点的邻居。这意味着在某个核心点的这个半径内的点属于同一个簇。选择适当的 eps 值至关重要，因为非常小的 eps 值可能导致太多的小簇，而非常大的 eps 值可能会将不同的簇合并成一个。

**min\_samples** 参数决定了形成一个密集区域所需的最少数据点数。如果一个点在 eps 半径内至少有 min\_samples 个邻居，它就被分类为一个**核心点**。如果一个点位于核心点的 eps 半径内但其自身未达到 min\_samples 阈值，它被分类为一个**边界点**。任何既不是核心点也不是边界点的点被标记为噪音或异常点。

### DBSCAN 如何对数据点进行分组

DBSCAN 通过识别核心点并围绕它们扩展簇来运行。它基于密度将紧密聚集的点（或簇）分组，将低密度点标记为异常（或噪声）。过程如下：

1.  **选择一个未访问的点**，并检查它在 `eps` 半径内是否至少有 `min_samples` 个邻居。
    
2.  如果有，该点成为一个**核心点**，并在其周围形成一个新的簇。
    
3.  **扩展簇**，加入所有 `eps` 范围内直接可达的点。如果这些点中有核心点，它们的邻居也将被加入。
    
4.  **继续扩展**，直到没有更多的点符合密度标准。
    
5.  **移动到下一个未访问的点**，重复该过程。
    
6.  **将剩余的点分类**为边界点（属于簇但不是核心点）或噪声（不属于任何簇的异常值）。
    

### DBSCAN 的实现示例

在此实现中：

-   `eps=0.3`：定义了点要被视为邻居的接近程度。
    
-   `min_samples=5`：设置了形成密集区域所需的最少点数。
    
-   `fit_predict(X)`：为每个数据点分配一个簇标签。
    

应用 DBSCAN 后，数据点被分配了标签。如果两个点属于同一簇，它们在 `y_db` 中将有相同的标签。被识别为异常的点将被标记为 `-1` 且不被聚类。

结果散点图直观地展示了 DBSCAN 是如何识别出两个新月形簇的。与假设球形簇的 K-Means 不同，DBSCAN 能够有效地检测任意形状的簇。

```
plt.scatter(X[y_db == 0, 0], X[y_db == 0, 1],
            c='lightblue', marker='o', s=40,
            edgecolor='black', 
            label='cluster 1')
plt.scatter(X[y_db == 1, 0], X[y_db == 1, 1],
            c='red', marker='s', s=40,
            edgecolor='black', 
            label='cluster 2')
plt.legend()
plt.show()
```

![Python代码截图，用于使用Matplotlib绘制散点图。代码定义了两个具有不同颜色和标记的簇，添加了图例，并显示了图。 - lunartech.ai](https://cdn.hashnode.com/res/hashnode/image/upload/v1738529515628/a5c2861e-1263-4cad-84f2-9e026261942f.png)

![散点图显示两个簇：簇1由顶部形成曲线的绿色圆圈构成，簇2由下方形成曲线的红色方块组成。图片来源：作者](https://miro.medium.com/v2/resize:fit:1400/1*ymoTCnR3H-WBs8ShoTrYNg.png)

生成的图将显示两个绿色和红色的新月形簇，展示 DBSCAN 成功地识别并分离了两个交错的半圆。

![插图展示了一张图解，未贴标签的数据点在不同图表上被聚类到两个不同的分组中。一个火柴人问，“如何在没有标签的情况下评估这些？” - lunartech.ai](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9a97d1f6-3c00-4493-b430-1d8e3cb8d270_3327x1350.png)

## **如何评估聚类算法的性能**

评估聚类模型的性能可能具有挑战性，因为在无监督学习中没有可用的真实标签。但是，有几种评估指标可以提供有关聚类结果质量的见解。

我建议您结合多种评估指标和可视化评估来全面评估聚类模型的性能。

## **K-Means、层次聚类和 DBSCAN 的区别**

K-Means、层次聚类和 DBSCAN 是三种广泛使用的聚类算法，各自有其分组数据点的方法。理解它们之间的差异对于根据数据特征和分析目标选择最合适的方法至关重要。

### **K-Means 聚类**

K-Means 聚类是一种基于质心的算法，根据相似性将数据划分为 K 个簇。该算法首先随机初始化 K 个质心，然后迭代地将每个数据点分配到最近的质心。一旦所有数据点都被分配完成，质心将根据每个簇内点的均值重新计算。这个过程一直持续到收敛为止。

#### **K-Means 聚类的优点：**

-   对于大型数据集，高效且具有良好的扩展性。
    
-   在簇呈现球形且均匀分布时效果良好。
    
-   与层次聚类相比计算速度更快。
    
-   易于实施和解释。
    

#### **K-Means 聚类的缺点：**

-   需要提前指定簇的数量（K）。
    
-   对初始质心位置敏感，导致结果可变。
    
-   假设簇大小相等且为球形，而这并不总是如此。
    
-   难以处理离群点和非线性形状的簇。
    

### **层次聚类**

层次聚类创建了一个嵌套的簇层次结构，无需预先定义簇数。它开始时将每个数据点视为一个单独的簇，并根据相似性逐步合并或拆分簇。结果通常使用树状图进行可视化，这有助于确定最佳簇数。

#### **层次聚类的优点：**

-   **不**需要提前指定簇数。
    
-   捕获簇之间的层次关系。
    
-   能处理包括数值和分类数据在内的不同类型的数据。
    
-   与树状图一起进行探索性分析时更易解释。
    

#### **层次聚类的缺点：**

-   对于大型数据集计算成本高（O(n²)复杂度）。
    
-   由于内存限制，处理大量数据点时难以扩展。
    
-   为树状图选择正确的截断点可能具有挑战性。
    
-   对噪声和离群点敏感，可能会扭曲层次结构。
    

### **DBSCAN（基于密度的空间聚类算法，带噪声）**

DBSCAN 是一种基于密度的聚类算法，通过数据点的接近度和密度而非预定义的簇将其分组。与 K-Means 和层次聚类不同，DBSCAN 不需要指定簇数。相反，它使用两个关键参数：eps（两个点被认为是邻居的最大距离）和 min_samples（形成密集簇所需的最小点数）。不满足这些标准的点被分类为噪声。

#### **DBSCAN 的优点：**

-   不需要提前指定簇数。
    
-   可以检测任意形状的簇，不像 K-Means 假设簇为球形。
    
-   有效处理离群点，将它们标记为噪声而不是强行归入某个簇。
    
-   适用于具有不同密度和非线性结构的数据集。
    

#### **DBSCAN 的缺点：**

-   处理不同的簇密度有难度，因为单一的 eps 值可能不适合所有簇。
    
-   对参数调整（eps 和 min_samples）敏感，可能影响聚类性能。
    
-   不适合高维数据，因为在高维空间中欧几里得距离失去意义。
    
-   可能在处理非常大型数据集时遇到问题，但比层次聚类扩展性更好。
    

### **选择合适的聚类算法**

| 特征 | K-Means | 层次聚类 | DBSCAN |
| --- | --- | --- | --- |
| **簇形状** | 假设簇为球形 | 适用于层次结构 | 处理任意形状簇 |
| **扩展性** | 非常可扩展（对大数据集快速） | 不可扩展（O(n²)复杂度） | 中等可扩展性（在非常大数据集时可能会遇到问题） |
| **簇数量** | 必须预定义 | 无需指定 | 无需指定 |
| **处理离群点** | 差 | 对噪声敏感 | 良好，将离群点检测为噪声 |
| **计算复杂度** | O(n) 到 O(n log n) | O(n²) | O(n log n) |
| **可解释性** | 结果易于解释 | 树状图提供了良好的见解 | 较不直观，需要调整参数 |

每种聚类算法各有其优劣。**K-Means** 在处理大型数据集且簇是球形且分离良好时非常理想。**层次聚类** 在存在层次关系或簇的数量未知时非常有用。**DBSCAN** 在检测任意形状的簇和处理噪声方面表现出色，但需要仔细调整参数。

[![具有50个困惑度的t-SNE可视化，显示数据点的聚类。标记的聚类突出显示了各个年份、分数和电影类型，如浪漫、惊悚、动作和冒险。 - lunartech.ai](https://miro.medium.com/v2/resize:fit:1400/1*HpMauXQZe0ByFFSHs4wNLw.png)][41]

## **如何使用 Python 的 t-SNE 进行聚类可视化**

在应用 K-Means、层次聚类和DBSCAN等聚类算法之后，你常常需要对结果进行可视化，以便更好地理解底层数据结构。

虽然散点图对二维或三维数据集效果很好，但现实世界的数据集通常包含高维特征，难以直观呈现。

为了解决这个问题，你可以使用降维技术，如**t-SNE**（t-分布随机邻居嵌入），将高维数据投射到低维空间，同时保留其结构。这让你更有效地可视化聚类，并识别原始数据中不易察觉的隐藏模式。

在本节中，我们将探讨 t-SNE 的理论及其在 Python 中的实现。

### **理解 t-SNE**

t-SNE 由 Laurens van der Maaten 和 Geoffrey Hinton 于 2008 年引入，作为可视化复杂数据结构的方法。它旨在在低维空间中表示高维数据点，同时保留数据点之间的局部结构和成对相似性。

t-SNE 通过在高维空间和低维空间中建模数据点之间的相似性来实现这一目标。

### **t-SNE 算法**

t-SNE 算法按以下步骤进行：

1.  计算高维空间中数据点之间的成对相似性。通常使用高斯核，根据数据点之间的欧氏距离测量相似性。

2.  随机初始化低维嵌入。

3.  定义一个成成本函数，代表高维空间和低维空间中数据点之间的相似性。

4.  使用梯度下降优化成本函数，以最小化高维和低维相似性之间的差异。

5.  迭代步骤3和4，直到成本函数收敛。

在 Python 中实现 t-SNE 相对简单，借助库如 scikit-learn。scikit-learn 为你的数据应用 t-SNE 提供了用户友好的 API。通过遵循 scikit-learn 的文档和示例，你可以轻松地将 t-SNE 融入到你的机器学习流程中。

### **2D t-SNE 可视化**

```
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.manifold import TSNE

# 加载数据集
digits = datasets.load_digits()
X, y = digits.data, digits.target

# 应用 t-SNE
tsne = TSNE(n_components=2, random_state=0)
X_tsne = tsne.fit_transform(X)

# 在二维平面上可视化结果
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, edgecolor='none', alpha=0.7, cmap=plt.cm.get_cmap('jet', 10))
plt.colorbar(scatter)
plt.title("t-SNE of Digits Dataset")
plt.show()
```

[![Python 代码片段，使用 Matplotlib 和 scikit-learn 可视化数字数据集的 t-SNE 转换。代码加载数据集，应用 t-SNE，并在二维平面上绘制结果。 - lunartech.ai](https://cdn.hashnode.com/res/hashnode/image/upload/v1738529609503/e4a5dac2-0c31-4e9c-b8cd-9d243736ee67.png)][42]

[![散点图显示数字数据集的 t-SNE 可视化。彩色点的聚类代表不同的数字，颜色从深红到浅蓝，对应于0到9的数字。右侧有一个颜色条，指示每种颜色代表的数字。 - lunartech.ai](https://miro.medium.com/v2/resize:fit:1400/1*vFccfsJFgXl3rulHs93MKA.png)][43]

在这个例子中：

1.  我们加载了`digits`数据集。

2.  我们应用 t-SNE 将数据从64维（因为每个图像是8x8）降到2维。

3.  然后我们绘制转换后的数据，每个点根据其真实的数字标签着色。

结果的可视化将显示聚类，每个聚类对应一个数字（0到9）。这有助于理解在原始高维空间中不同数字的分离程度。

### **可视化高维数据**

t-SNE 的主要优势之一是能够在低维空间中可视化高维数据。通过降低数据的维度，t-SNE 使我们能够识别在原始高维空间中不明显的聚类和模式。所得的可视化可以为数据结构提供有价值的见解，并有助于决策过程。

```
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.manifold import TSNE
from mpl_toolkits.mplot3d import Axes3D

# 加载数据集
digits = datasets.load_digits()
X, y = digits.data, digits.target

# 应用 t-SNE
tsne = TSNE(n_components=3, random_state=0)
X_tsne = tsne.fit_transform(X)

# 在三维平面上可视化结果
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], c=y, edgecolor='none', alpha=0.7, cmap=plt.cm.get_cmap('jet', 10))
plt.colorbar(scatter)
plt.title("3D t-SNE of Digits Dataset")
plt.show()
```

[![Python 中使用 matplotlib、sklearn 和 mpl_toolkits.mplot3d 的库的代码片段。它加载数字数据集，应用t-SNE进行降维，并在三维平面上可视化结果。 - lunartech.ai](https://cdn.hashnode.com/res/hashnode/image/upload/v1738529676545/772f6b94-655b-4ae3-bdb5-a5334442c970.png)][44]

在这个修改后的代码中：

1.  我们为 t-SNE 设置`n_components=3`以获得三维转换。

2.  我们使用`mpl_toolkits.mplot3d.Axes3D`创建三维散点图。

执行代码后，你会看到一个三维散点图，数据点根据其 t-SNE 坐标定位，并根据其真实数字标签着色。

旋转三维可视化有助于我们更好地理解数据点的空间分布。

[![数字数据集的 t-SNE 投影的 3D 散点图。数据点以各种颜色分组，代表不同的数字。右侧有一个颜色条，指示0到9的数字值。](https://miro.medium.com/v2/resize:fit:1400/1*aw8wAIvC2CXwXO7Ixjy1JQ.png)][45]

t-SNE 是降维和高维数据可视化的强大工具。通过利用其能力，你可以更深入地理解复杂数据集，并发现可能不明显的隐藏模式。借助其 Python 实现和易用性，t-SNE 对任何数据科学家或机器学习从业者来说都很有价值。

[![散点矩阵图，显示sepal宽度、sepal长度、petal宽度和petal长度之间的关系，针对iris物种：setosa（蓝色）、versicolor（红色）和virginica（绿色）。 - lunartech.ai](https://cdn-images-1.medium.com/max/1600/1*sNHaMTQBe3plUhk3k2dnYg.gif)][46]

## **更多无监督学习技术**

除了我们在此讨论的聚类技术外，还有一些重要的无监督学习技术值得探索。虽然我们在这里不会详细探讨，但让我们简要提及两种技术：混合模型和主题建模。

### **混合模型**

混合模型是用于建模复杂数据分布的概率模型。它们假定整个数据集可以描述为多个潜在亚群或成分的组合，每个成分由其自身的概率分布描述。

在数据点不明显属于不同类群且可能具有重叠特征的情况下，混合模型尤其有用。

### **主题建模**

主题建模是一种从文档集合中提取潜在主题的技术。它允许你探索和发现文本数据中的潜在语义模式。

通过分析跨文档的词共现并识别常见主题，主题建模实现了大规模文本数据集的自动分类和摘要。该技术在自然语言处理、信息检索和内容推荐系统等领域有应用。

虽然这些技术超出了本手册的范围，但它们是揭示隐藏模式和从数据中获取洞见的重要工具。

记住，掌握无监督学习需要不断学习和实践。通过熟悉像上面提到的不同技术，你将能很好地应对各个领域的广泛数据分析问题。

## **常见问题解答**

### **问：监督学习和无监督学习有什么区别？**

监督学习涉及在标记数据上训练模型，其中输入与对应输出配对。目标是预测新、看不见输入的输出。

相反，无监督学习处理未标记的数据，其目标是在没有预定义输出的情况下发现数据中的模式、结构或聚类。

本质上，监督学习旨在学习映射函数，而无监督学习则专注于揭示数据中的隐藏关系或分组。

### **问：哪个聚类算法最适合我的数据？**

聚类算法的适用性取决于多种因素，例如数据的性质、所需的聚类数量及你所尝试解决的具体问题。

在本手册中，我们讨论了三种常用的聚类算法：

-   **K-means**是一种旨在将数据分为K个聚类的流行算法，每个数据点分配到最近的质心。适用于均匀分布、球形聚类，需要事先指定聚类数量。
    
-   **层次聚类**通过迭代合并或分裂它们来构建聚类层次结构。它提供了一个树状图来可视化聚类过程，可以处理不同形状和大小的聚类。
    
-   **DBSCAN**是一种基于密度的算法，将彼此靠近的数据点分组，并分隔离群点。它可以发现任意形状的聚类，不需要预先知道聚类数量。
    

要确定你的用例的最佳算法，我建议你尝试不同的技术，并基于聚类质量、计算效率和可解释性等指标评估其性能。

### **问：能否将无监督学习用于预测分析？**

虽然无监督学习主要专注于发现数据中的模式和关系，而无需具体的输出标签，但它可以间接支持预测分析。通过揭示数据中的隐藏结构和聚类，无监督学习可以提供洞见，促进更好的特征工程、异常检测或分割，从而增强预测模型的性能。

无监督学习技术如聚类可以帮助识别数据中的不同组或模式，这些可以用作预测模型的输入特征或用作生成新预测变量的基础。因此，通过促进更深的**了解数据，无监督学习在预测分析中扮演了一个重要角色，提升了预测模型的准确性和有效性。**

## **数据科学和AI资源**

想了解更多关于数据科学、机器学习和AI职业，以及如何获得数据科学工作的知识？你可以下载这本[免费的数据科学和AI职业手册][47]。

想从头学习机器学习，或刷新你的记忆？下载这本[免费的机器学习基础手册][48]，将所有机器学习基础与Python示例结合到了一起。

## **关于作者**

[**Tatev Aslanyan**][49]是一位高级机器学习和AI工程师、CEO和[**LunarTech**][50]的联合创始人，这是一家致力于在全球范围内普及数据科学和AI的深度科技创新初创企业。拥有超过6年的AI工程和数据科学经验，Tatev曾在美国、英国、加拿大和荷兰工作，运用她的专业知识推进各行各业的AI解决方案。

Tatev拥有荷兰顶尖大学的[经济计量学和运筹学的硕士和学士学位][51]，并在自然语言处理（NLP）、机器学习和推荐系统方面发表了多篇科学论文，这些论文在美国的知名科学期刊上发表。

作为顶级开源贡献者，Tatev参与编写了课程和书籍，包括**freeCodeCamp 2024**的资源，并在[**LunarTech**的项目][53]中，通过其教育超过**144个国家的30,000多名学习者。**

[LunarTech][54]是一家深度科技创新公司，开发AI驱动的产品并提供教育工具，以帮助企业和个人进行创新，降低运营成本和提高盈利能力。

## **与我们联系**

-   [在LinkedIn上与我联系][55]
    
-   [查看YouTube频道][56]
    
-   订阅[**LunarTech通讯**][57]或[**LENS**][58] - 我们的新闻频道
    

想了解有关数据科学、机器学习和AI职业的所有信息，并学习如何获得数据科学工作？下载这本免费的数据科学和AI职业手册。

感谢选择本指南作为你的学习伴侣。随着你继续探索广泛的人工智能领域，希望你带着信心、精准和创新精神进行。

## **LunarTech的AI工程训练营**

如果你认真考虑成为AI工程师，并想要一个结合深厚理论与实战训练的全能训练营，那就查看[**LunarTech AI工程训练营**][59]，专注于生成AI。 这个AI工程综合高级项目，旨在为你提供在最具竞争力的AI角色和行业中茁壮成长所需的一切。

<iframe width="560" height="315" src="https://www.youtube.com/embed/g6KQHEeZVQY" style="aspect-ratio: 16 / 9; width: 100%; height: auto;" title="YouTube视频播放器" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" loading="lazy"></iframe>

在3到6个月的自定进度或基于小组的学习中，你将学习生成AI和基础模型，如VAEs、GANs、transformers和LLMs。深入了解数学、统计、架构，以及使用行业标准框架如 PyTorch 和 TensorFlow 训练这些模型的技术细节。

课程内容包括大型模型的预训练、微调、提示工程、量化和优化，以及诸如检索增强生成（RAGs）等尖端技术。

这个训练营帮助你弥合研究和实际应用之间的差距，使你能够设计出有影响的解决方案，同时建立一个包含高级项目丰富的出色作品集。

该课程还优先考虑AI伦理，培训你创建与负责人工智能原则一致的可持续伦理模型。这不仅仅是另外一门课程——这是一个全面的旅程，旨在让你成为AI革命的领军人物。[查看课程大纲][60]

名额有限，对熟练AI工程师的需求比以往任何时候都高。不要等待——你的AI工程师职业未来从现在开始。你可以[此处申请][61]。

> “让我们一起构建未来！” - Tatev Aslanyan，LunarTech 的 CEO 和联合创始人

## [**数据科学与AI通讯 | Tatev Karen | Substack**][62]

想从头学习机器学习或者刷新你的记忆？下载这本[**免费的机器学习基础手册**][63]

想了解关于数据科学、机器学习和AI职业的所有内容，以及如何获得数据科学工作？下载这本[**免费的数据科学和AI职业手册**][64]。

感谢选择本指南作为你的学习伴侣。 随着你继续探索广泛的机器学习领域， 希望你带着信心、精准和创新精神进行。 祝你在所有未来的努力中一切顺利！

